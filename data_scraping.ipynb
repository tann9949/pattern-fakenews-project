{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20gMXQsdl-D8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heP9YtGgFkGS"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjFZTsRkFeTK"
      },
      "outputs": [],
      "source": [
        "import pythainlp\n",
        "from pythainlp import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDE0YfyX8xaF"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import urllib\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from webdriver_manager.firefox import GeckoDriverManager"
      ],
      "metadata": {
        "id": "E1bUbGhTkT-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VycNMJcrCrPv"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(x):\n",
        "    output = word_tokenize(x)\n",
        "    for i in reversed(range(len(output))):\n",
        "        if output[i].strip() == \"\":\n",
        "            del output[i]\n",
        "        else:\n",
        "            output[i] = output[i].strip()\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQqt8giOCrSJ"
      },
      "outputs": [],
      "source": [
        "def preprocess_list(x):\n",
        "    output = []\n",
        "    x = x[:-4]\n",
        "    for sentence in x:\n",
        "        output += preprocess_sentence(sentence)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExnqKVC0CrU9"
      },
      "outputs": [],
      "source": [
        "def process_data(file_path, out_path):\n",
        "    df = pd.read_json(path_or_buf=file_path, lines=True)\n",
        "    \n",
        "    df[\"Title\"] = df[\"Title\"].apply(preprocess_sentence)\n",
        "    df[\"Detail\"] = df[\"Detail\"].apply(preprocess_list)\n",
        "\n",
        "    df[\"Document Tag\"] = \"Fact News\"\n",
        "\n",
        "    df['json'] = df.apply(lambda x: x.to_json(), axis=1)\n",
        "    \n",
        "    file_data = open(out_path, \"w\", encoding=\"utf8\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        file_data.write(row[\"json\"]+\"\\n\")  \n",
        "\n",
        "    file_data.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeKNg3azMvn6"
      },
      "source": [
        "## kapook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_kapook_url():\n",
        "    base_url = \"https://health.kapook.com/news\"\n",
        "    \n",
        "    firefox_options = webdriver.FirefoxOptions()\n",
        "    firefox_options.add_argument('--headless')\n",
        "    firefox_options.add_argument('--no-sandbox')\n",
        "    firefox_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Firefox(executable_path=GeckoDriverManager().install(), options=firefox_options)\n",
        "    driver.get(base_url)\n",
        "\n",
        "    loadmore = driver.find_element(by=By.XPATH , value=\"//a[@id='loadmore']\")\n",
        "    j = 0\n",
        "\n",
        "    while loadmore.is_displayed():\n",
        "        try:\n",
        "            loadmore.click()\n",
        "            time.sleep(0.5)\n",
        "            if j%100 == 0:\n",
        "                print(\"-- {} --\".format(j))\n",
        "                f = open(\"kapook_url.txt\", \"a\")\n",
        "                html_source = driver.page_source\n",
        "                soup = BeautifulSoup(html_source, 'html.parser')\n",
        "                soup = soup.find(\"ul\", {\"class\": \"hits2\"})\n",
        "                blogs = soup.findAll(\"a\")\n",
        "\n",
        "                for blog in blogs:\n",
        "                    url = blog[\"href\"]\n",
        "                    f.write(url.strip()+\"\\n\")\n",
        "                f.close()  \n",
        "\n",
        "            j += 1\n",
        "            \n",
        "            loadmore = driver.find_element(by=By.XPATH , value=\"//a[@id='loadmore']\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    try:\n",
        "        html_source = driver.page_source\n",
        "        driver.quit()\n",
        "        soup = BeautifulSoup(html_source, 'html.parser')\n",
        "        soup = soup.find(\"ul\", {\"class\": \"hits2\"})\n",
        "        blogs = soup.findAll(\"a\")\n",
        "        print(len(urls))\n",
        "        f = open(\"kapook_url_done.txt\", \"a\")             \n",
        "        for blog in blogs:\n",
        "            url = blog[\"href\"]\n",
        "            f.write(url.strip()+\"\\n\")\n",
        "        f.close()\n",
        "    \n",
        "    except:\n",
        "        html_source = driver.page_source\n",
        "        driver.quit()\n",
        "        f.write(html_source)    "
      ],
      "metadata": {
        "id": "9kDi4oIRkPV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_duplicate_kapook(file_url):\n",
        "    url = set()\n",
        "    f = open(file_url, \"r\")\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line == \"\":\n",
        "            continue\n",
        "            \n",
        "        _url = line.split(\"view\")[1][:-5]\n",
        "        url.add(_url)\n",
        "    f.close()\n",
        "    \n",
        "    print(\"There are {} urls\".format(len(url)))\n",
        "    \n",
        "    out = open(\"out_url_process.txt\", \"a\")\n",
        "    for _url in url:\n",
        "        out.write(_url + \"\\n\")\n",
        "    out.close()\n",
        "        \n",
        "    return url"
      ],
      "metadata": {
        "id": "dj9Pzj_ulONB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czJf-pkKMzjG"
      },
      "outputs": [],
      "source": [
        "def scrape_kapook_data():\n",
        "    file_url = open(\"/content/drive/MyDrive/Pattern/out_url_process.txt\", \"r\", encoding=\"utf8\")\n",
        "    file_data = open(\"/content/drive/MyDrive/Pattern/kapook_data.json\", \"w\", encoding=\"utf8\")\n",
        "    base_url = \"https://health.kapook.com/view{}.html\"\n",
        "    \n",
        "    for url in tqdm(file_url):\n",
        "        try:\n",
        "            url = url.strip()\n",
        "            _res = urllib.request.urlopen(base_url.format(url))\n",
        "            res = _res.read()\n",
        "            res = res.decode(\"utf8\")\n",
        "            soup = BeautifulSoup(res, 'html.parser')\n",
        "            _res.close()\n",
        "    \n",
        "            title = soup.find(\"h1\", {\"itemprop\": \"headline\"})\n",
        "            title = title.get_text().strip()    \n",
        "                              \n",
        "            content = []\n",
        "            soup = soup.find(\"div\", {\"class\": \"content\"})\n",
        "            \n",
        "            for s in soup(\"a\"):\n",
        "                s.extract()\n",
        "\n",
        "            _content = soup.get_text()\n",
        "\n",
        "            for _ in _content.split(\" \"):\n",
        "                _text = _.strip()\n",
        "                if _text == \"\":\n",
        "                    continue\n",
        "                elif \"เรียบเรียงข้อมูลโดย\"in _text or \"ขอขอบคุณภาพประกอบจาก\" in _text:\n",
        "                    continue\n",
        "                elif \"อ่านรายละเอียดเพิ่มเติมจาก\" in _text:\n",
        "                    _text = _text[:_text.index(\"อ่านรายละเอียดเพิ่มเติมจาก\")]\n",
        "                    content.append(_text)\n",
        "                else:\n",
        "                    content.append(_text) \n",
        "\n",
        "            data = {\n",
        "                \"Title\": title,\n",
        "                \"Detail\": content\n",
        "            }\n",
        "\n",
        "            json_string = json.dumps(data, ensure_ascii=False)\n",
        "            \n",
        "            file_data.write(json_string+\"\\n\")   \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(\"\\n\")\n",
        "            print(url)\n",
        "            print(e)\n",
        "            continue\n",
        "        \n",
        "    file_data.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8MwTFocMzlH"
      },
      "outputs": [],
      "source": [
        "scrape_kapook_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhLrDRplMznr"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Pattern/kapook_data.json\"\n",
        "out_path = \"/content/drive/MyDrive/Pattern/dataset/kapook_dataset.json\"\n",
        "# process_data(file_path, out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvRTlv32MzqK"
      },
      "outputs": [],
      "source": [
        "check = pd.read_json(path_or_buf=out_path, lines=True)\n",
        "check.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCA76bdut-eD"
      },
      "source": [
        "## sanook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_sanook_url():\n",
        "    base_url = \"https://www.sanook.com/health/\"\n",
        "    \n",
        "    firefox_options = webdriver.FirefoxOptions()\n",
        "    firefox_options.add_argument('--headless')\n",
        "    firefox_options.add_argument('--no-sandbox')\n",
        "    firefox_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Firefox(executable_path=GeckoDriverManager().install(), options=firefox_options)\n",
        "    driver.get(base_url)\n",
        "\n",
        "    driver.find_element(by=By.XPATH , value=\"//button[@class='jsx-1854747484 button']\").click()\n",
        "    loadmore = driver.find_element(by=By.XPATH , value=\"//button[@class='jsx-3493116903 bg-color-health pagination typeDefault']\")\n",
        "    j = 0\n",
        "\n",
        "    while loadmore.is_displayed():\n",
        "        try:\n",
        "            loadmore.click()\n",
        "            time.sleep(0.5)\n",
        "            if j%100 == 0:\n",
        "                f = open(\"sanook_url_py.txt\", \"a\")\n",
        "                f.write(\"\\n\")\n",
        "                html_source = driver.page_source\n",
        "                soup = BeautifulSoup(html_source, 'html.parser')\n",
        "                urls = soup.findAll(\"a\", {\"class\": \"jsx-1104899621 EntryListImage\"})\n",
        "                print(\"-- {} --\".format(j))\n",
        "                print(len(urls))\n",
        "\n",
        "                for _url in urls:\n",
        "                    url = _url[\"href\"]\n",
        "                    f.write(url.strip()+\"\\n\")\n",
        "                f.close()  \n",
        "\n",
        "            j += 1\n",
        "            \n",
        "            loadmore = driver.find_element(by=By.XPATH , value=\"//button[@class='jsx-3493116903 bg-color-health pagination typeDefault']\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            break\n",
        "    \n",
        "    try:\n",
        "        html_source = driver.page_source\n",
        "        driver.quit()\n",
        "        soup = BeautifulSoup(html_source, 'html.parser')\n",
        "        \n",
        "        urls = soup.findAll(\"a\", {\"class\": \"jsx-1104899621 EntryListImage\"})\n",
        "        print(len(urls))\n",
        "        f = open(\"sanook_url_done_py.txt\", \"a\")  \n",
        "        f.write(\"\\n\")           \n",
        "        for _url in urls:\n",
        "            url = _url[\"href\"]\n",
        "            f.write(url.strip()+\"\\n\")\n",
        "        f.close()\n",
        "    \n",
        "    except:\n",
        "        html_source = driver.page_source\n",
        "        driver.quit()  "
      ],
      "metadata": {
        "id": "tQksRBMlkYzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuKgJ0ca9xKe"
      },
      "outputs": [],
      "source": [
        "def clean_duplicate(file_url):\n",
        "    url = set()\n",
        "    f = open(file_url, \"r\")\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line == \"\":\n",
        "            continue\n",
        "            \n",
        "        _url = line[30: -1]\n",
        "        url.add(_url)\n",
        "    \n",
        "    print(\"There are {} usls\".format(len(url)))\n",
        "        \n",
        "    return url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLYnEdox9veH"
      },
      "outputs": [],
      "source": [
        "def scrape_sanook_data():\n",
        "    file_url = \"/content/drive/MyDrive/Pattern/sanook_url_colab.txt\"\n",
        "    urls = clean_duplicate(file_url)\n",
        "    file_data = open(\"/content/drive/MyDrive/Pattern/dataset/sanook_data.json\", \"w\", encoding=\"utf8\")\n",
        "    base_url = \"https://www.sanook.com/health/{}/\"\n",
        "    \n",
        "    for url in tqdm(urls):\n",
        "        try:\n",
        "            _res = urllib.request.urlopen(base_url.format(url))\n",
        "            res = _res.read()\n",
        "            res = res.decode(\"utf8\")\n",
        "            soup = BeautifulSoup(res, 'html.parser')\n",
        "            _res.close()\n",
        "    \n",
        "            title = soup.find(\"h1\", {\"class\": \"jsx-2761676397 title\"})\n",
        "            title = title.get_text().strip()                \n",
        "                              \n",
        "            content = []\n",
        "            soup = soup.find(\"div\", {\"class\": \"jsx-3647499928 jsx-3717305904\"})\n",
        "            _content = soup.findAll([\"p\", \"h3\", \"li\"])\n",
        "            for _ in _content:\n",
        "                if _.text.strip() == \"\":\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(_.text.strip()) \n",
        "\n",
        "            data = {\n",
        "                \"Title\": title,\n",
        "                \"Detail\": content\n",
        "            }\n",
        "\n",
        "            json_string = json.dumps(data, ensure_ascii=False)\n",
        "            \n",
        "            file_data.write(json_string+\"\\n\")     \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(\"\\n\")\n",
        "            print(url)\n",
        "            print(e)\n",
        "            continue\n",
        "        \n",
        "    file_data.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vwmu1lTb9zK_"
      },
      "outputs": [],
      "source": [
        "scrape_sanook_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF9dK5n1Cvkg"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Pattern/dataset/sanook_data.json\"\n",
        "out_path = \"/content/drive/MyDrive/Pattern/dataset/sanook_dataset.json\"\n",
        "process_data(file_path, out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk76k0s3Cvm2"
      },
      "outputs": [],
      "source": [
        "check = pd.read_json(path_or_buf=out_path, lines=True)\n",
        "check.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH3nvGk7noJe"
      },
      "outputs": [],
      "source": [
        "check.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erm2wxBS7kKp"
      },
      "outputs": [],
      "source": [
        "def scrape_sanook_url():\n",
        "    base_url = \"https://www.sanook.com/health/\"\n",
        "\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "    driver.get(base_url)\n",
        "\n",
        "    driver.find_element(by=By.XPATH , value=\"//button[@class='jsx-1854747484 button']\").click()\n",
        "    loadmore = driver.find_element(by=By.XPATH , value=\"//button[@class='jsx-3493116903 bg-color-health pagination typeDefault']\")\n",
        "    j = 0\n",
        "\n",
        "    while loadmore.is_displayed():\n",
        "        try:\n",
        "            loadmore.click()\n",
        "            time.sleep(0.5)\n",
        "            if j%100 == 0:\n",
        "                print(\"-- {} --\".format(j))\n",
        "\n",
        "                if j > 1000:\n",
        "                    f = open(\"sanook_url_py.txt\", \"a\")\n",
        "                    f.write(\"\\n\")\n",
        "                    html_source = driver.page_source\n",
        "                    soup = BeautifulSoup(html_source, 'html.parser')\n",
        "                    urls = soup.findAll(\"a\", {\"class\": \"jsx-1104899621 EntryListImage\"})\n",
        "                \n",
        "\n",
        "                    for _url in urls:\n",
        "                        url = _url[\"href\"]\n",
        "                        f.write(url.strip()+\"\\n\")\n",
        "                    f.close()  \n",
        "                    print(len(urls))\n",
        "                \n",
        "            j += 1\n",
        "            \n",
        "            loadmore = driver.find_element(by=By.XPATH , value=\"//button[@class='jsx-3493116903 bg-color-health pagination typeDefault']\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            break\n",
        "    \n",
        "    try:\n",
        "        html_source = driver.page_source\n",
        "        driver.quit()\n",
        "        soup = BeautifulSoup(html_source, 'html.parser')\n",
        "        \n",
        "        urls = soup.findAll(\"a\", {\"class\": \"jsx-1104899621 EntryListImage\"})\n",
        "        print(len(urls))\n",
        "        f = open(\"sanook_url_done_py.txt\", \"a\")  \n",
        "        f.write(\"\\n\")           \n",
        "        for _url in urls:\n",
        "            url = _url[\"href\"]\n",
        "            f.write(url.strip()+\"\\n\")\n",
        "        f.close()\n",
        "    \n",
        "    except:\n",
        "        html_source = driver.page_source\n",
        "        driver.quit()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCB-6xGnPw4Y"
      },
      "outputs": [],
      "source": [
        "scrape_sanook_url()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-3gQm5Bt35g"
      },
      "source": [
        "## matichon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fUO5HwZZT7z"
      },
      "outputs": [],
      "source": [
        "def scrape_matichon_url():\n",
        "    base_url = \"https://www.matichon.co.th/lifestyle/health-beauty\"\n",
        "    max_page = 98\n",
        "\n",
        "    f = open(\"matichon_url.txt\", \"a\")\n",
        "    for page in tqdm(range(0, max_page)):\n",
        "\n",
        "        if page == 0:\n",
        "            _res = urllib.request.urlopen(base_url)\n",
        "        else:\n",
        "            _res = urllib.request.urlopen(base_url+\"/page/\"+str(page+1))\n",
        "        res = _res.read()\n",
        "        res = res.decode(\"utf8\")\n",
        "        soup = BeautifulSoup(res, 'html.parser')\n",
        "        _res.close()\n",
        "        \n",
        "        try:\n",
        "            _ = soup.find(\"div\", {\"class\": \"td-pb-span8 td-main-content\"})\n",
        "            blogs = _.findAll(\"div\", {\"class\": \"td-module-thumb\"})\n",
        "                        \n",
        "            for blog in blogs:\n",
        "                url = blog.find(\"a\", {\"class\": \"ud-module-link\"})[\"href\"]\n",
        "                f.write(url.strip()+\"\\n\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(page+1)\n",
        "            continue\n",
        "\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j90NHkPOZUnU"
      },
      "outputs": [],
      "source": [
        "scrape_matichon_url()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_matichon_data():\n",
        "    file_url = open(\"/home/natthanon/pattern_project/matichon_url.txt\", \"r\")\n",
        "    file_data = open(\"/home/natthanon/pattern_project/matichon_data.json\", \"w\", encoding=\"utf8\")\n",
        "    for url in tqdm(file_url):\n",
        "        try:\n",
        "            _res = urllib.request.urlopen(url)\n",
        "            res = _res.read()\n",
        "            res = res.decode(\"utf8\")\n",
        "            soup = BeautifulSoup(res, 'html.parser')\n",
        "            _res.close()\n",
        "    \n",
        "            title = soup.find(\"h1\", {\"class\": \"entry-title\"})\n",
        "            title = title.get_text().strip()    \n",
        "            \n",
        "                              \n",
        "            content = []\n",
        "            soup = soup.find(\"div\", {\"itemprop\": \"articleBody\"})\n",
        "            _content = soup.findAll([\"p\"])\n",
        "            for _ in _content:\n",
        "                if _.findAll([\"a\"]):\n",
        "                    continue\n",
        "                elif _.text.strip() == \"\":\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(_.text.strip()) \n",
        "\n",
        "            data = {\n",
        "                \"Title\": title,\n",
        "                \"Detail\": content\n",
        "            }\n",
        "\n",
        "            json_string = json.dumps(data, ensure_ascii=False)\n",
        "            \n",
        "            file_data.write(json_string+\"\\n\")     \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "        \n",
        "    file_data.close()\n",
        "    file_url.close()"
      ],
      "metadata": {
        "id": "m3iiLknIlaw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDgF7x_PogV"
      },
      "source": [
        "## bbc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ9rKbAnPngR"
      },
      "outputs": [],
      "source": [
        "def scrape_bbc_url():\n",
        "    base_url = \"https://www.bbc.com/thai/topics/cyx5kz25zxdt/page/\"\n",
        "    _url = \"https://www.bbc.com\"\n",
        "    max_page = 100\n",
        "\n",
        "    f = open(\"bbc_url.txt\", \"a\")\n",
        "    for page in tqdm(range(max_page)):\n",
        "        res = requests.get(base_url+str(page+1))\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "        \n",
        "        try:\n",
        "            blogs = soup.findAll(\"a\", {\"class\": \"qa-story-cta-link\"})\n",
        "            for blog in blogs:\n",
        "                url = _url+blog[\"href\"]\n",
        "                f.write(url+\"\\n\")\n",
        "            \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWjtFLF2PsNg"
      },
      "outputs": [],
      "source": [
        "scrape_bbc_url()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrzo9zUAPsP7"
      },
      "outputs": [],
      "source": [
        "def scrape_bbc_data():\n",
        "    file_url = open(\"/content/bbc_url.txt\", \"r\")\n",
        "    file_data = open(\"/content/bbc_data.json\", \"w\", encoding=\"utf8\")\n",
        "    for url in tqdm(file_url):\n",
        "        try:\n",
        "            res = requests.get(url.strip())\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            soup = soup.find(\"main\", {\"role\": \"main\"})\n",
        "\n",
        "            title = soup.find(\"h1\", {\"id\": \"content\"})\n",
        "            title = title.get_text().strip()\n",
        "            print(title)\n",
        "            \n",
        "\n",
        "            content = []\n",
        "            _content = soup.findAll([\"p\", \"h2\", \"li\"])\n",
        "            for _ in _content:\n",
        "                if _.findAll([\"span\", \"a\"]):\n",
        "                    continue\n",
        "                elif _.text.strip() == \"\":\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(_.text.strip()) \n",
        "\n",
        "            data = {\n",
        "                \"title\": title,\n",
        "                \"content\": content\n",
        "            }\n",
        "\n",
        "            json_string = json.dumps(data, ensure_ascii=False)\n",
        "            file_data.write(json_string+\"\\n\")     \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "    file_data.close()\n",
        "    file_url.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEytaPEiPsSv"
      },
      "outputs": [],
      "source": [
        "scrape_bbc_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLrOM4kfHIOg"
      },
      "source": [
        "## pptvhd36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j9mazZbHHgG"
      },
      "outputs": [],
      "source": [
        "def scrape_pptvhd36_url():\n",
        "    base_url = \"https://www.pptvhd36.com/news/%E0%B8%AA%E0%B8%B8%E0%B8%82%E0%B8%A0%E0%B8%B2%E0%B8%9E?page=\"\n",
        "    max_page = 141\n",
        "\n",
        "    f = open(\"pptvhd36_url.txt\", \"a\")\n",
        "    for page in tqdm(range(max_page)):\n",
        "        res = requests.get(base_url+str(page+1))\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "        \n",
        "        _ = soup.findAll(\"div\", {\"class\": \"pptv-grid\"})\n",
        "        blogs = _[1].findAll(\"div\", {\"class\": \"pptv-col-3@m pptv-col-6@s\"})\n",
        "        try:\n",
        "            for blog in blogs:\n",
        "                url = blog.find(\"a\", {\"class\": \"content-item__thumb\"})[\"href\"]\n",
        "                f.write(url+\"\\n\")\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McOJvb0-HXzH"
      },
      "outputs": [],
      "source": [
        "# scrape_pptvhd36_url()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IphRfqiMUbPn"
      },
      "outputs": [],
      "source": [
        "def scrape_pptvhd36_data():\n",
        "    file_url = open(\"/content/drive/MyDrive/Pattern/pptvhd36_url.txt\", \"r\")\n",
        "    file_data = open(\"/content/drive/MyDrive/Pattern/pptvhd36_data.json\", \"w\", encoding=\"utf8\")\n",
        "    for url in tqdm(file_url):\n",
        "        try:\n",
        "            res = requests.get(url)\n",
        "            soup = BeautifulSoup(res.text, \"lxml\")\n",
        "\n",
        "            title = soup.find(\"h1\", {\"class\": \"section--head-line__title [ heading --large@m --small@s --tiny color-black bold ]\"})\n",
        "            title = title.get_text().strip()\n",
        "\n",
        "            subcontent = soup.find(\"div\", {\"class\": \"content-details__body\"})\n",
        "            subcontent = subcontent.find(\"section\", {\"class\": \"content-details__section section section--excerpt content-container color-black\"})\n",
        "            subcontent = subcontent.p.get_text().strip()\n",
        "\n",
        "            content = []\n",
        "            _content = soup.find(id=\"content-section\")\n",
        "            _content = _content.findAll([\"p\", \"li\"])\n",
        "            for _ in _content[:-1]:\n",
        "                if _.findAll([\"a\", \"section\"]):\n",
        "                    continue\n",
        "                elif _.text.strip() == \"\":\n",
        "                    continue\n",
        "                else:\n",
        "                    content.append(_.text.strip()) \n",
        "\n",
        "            data = {\n",
        "                \"title\": title,\n",
        "                \"subcontent\": subcontent,\n",
        "                \"content\": content\n",
        "            }\n",
        "\n",
        "            json_string = json.dumps(data, ensure_ascii=False)\n",
        "            file_data.write(json_string+\"\\n\")   \n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    return out\n",
        "        \n",
        "    file_data.close()\n",
        "    file_url.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_zSW-nFVRzI"
      },
      "outputs": [],
      "source": [
        "scrape_pptvhd36_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pptvhd36_data(file_path, out_path):\n",
        "    df = pd.read_json(path_or_buf=file_path, lines=True)\n",
        "    \n",
        "    df[\"Title\"] = df[\"title\"].apply(preprocess_sentence)\n",
        "    df[\"subcontent\"] = df[\"subcontent\"].apply(preprocess_sentence)\n",
        "    df[\"content\"] = df[\"content\"].apply(preprocess_list)\n",
        "\n",
        "    df[\"Detail\"] = df[\"subcontent\"].add(df[\"content\"])\n",
        "    df[\"Document Tag\"] = \"Fact News\"\n",
        "    df.drop([\"title\", \"subcontent\", \"content\"], axis=1, inplace=True)\n",
        "\n",
        "    df['json'] = df.apply(lambda x: x.to_json(), axis=1)\n",
        "    \n",
        "    file_data = open(out_path, \"w\", encoding=\"utf8\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        file_data.write(row[\"json\"]+\"\\n\")  \n",
        "\n",
        "    file_data.close()"
      ],
      "metadata": {
        "id": "m2GdjaF5lpmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NgXrjlzalpor"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_wDgF7x_PogV",
        "D5BiHxCfDQ85"
      ],
      "name": "Data Scraping.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}