{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed2d0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/samsung-4tb/cp-eng/pattern/course-projects/pattern-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q sentence_transformers protobuf matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dadd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "# %cd /samsung-4tb/cp-eng/pattern/course-projects/\n",
    "%cd /workspace\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# notebook lib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = \"dataset\"\n",
    "NEWS_DIR = f\"{DATA_DIR}/healthcare-news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c36ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(set_name):\n",
    "    if set_name.lower().strip() == \"wordpiece\":\n",
    "        json_paths = sorted(glob(f\"{NEWS_DIR}/wordpiece/*.json\"))\n",
    "        return {\n",
    "            os.path.basename(f_name).split(\"_\")[0]: [\n",
    "                json.loads(line.strip())\n",
    "                for line in open(f_name).readlines()\n",
    "            ]\n",
    "            for f_name\n",
    "            in tqdm(json_paths)\n",
    "        }\n",
    "    elif set_name.lower().strip() == \"sentencepiece\":\n",
    "        return [l.strip() for l in open(f\"{NEWS_DIR}/sentencepiece/sentencepiece.txt\")]\n",
    "    elif set_name.lower().strip() == \"raw\":\n",
    "        return [l.strip() for l in open(f\"{NEWS_DIR}/healthcare-raw.txt\")]\n",
    "\n",
    "\n",
    "def save_raw_text(raw_text, save_path):\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.writelines([re.sub(r\" +\", \" \", \" \".join(t))+\"\\n\" for t in raw_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1de8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8178986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All characters:\n",
      "\n",
      "\" \" \"!\" \"\"\" \"#\" \"%\" \"&\" \"'\" \"(\" \")\" \"*\" \"+\" \",\" \"-\" \".\" \"/\" \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \":\" \";\" \"<\" \"=\" \">\" \"?\" \"@\" \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\" \"[\" \"\\\" \"]\" \"^\" \"_\" \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\" \"{\" \"|\" \"}\" \"~\" \"¬ç\" \"¬é\" \"¬ù\" \"¬û\" \"¬†\" \"¬¨\" \"¬≠\" \"¬Æ\" \"¬∞\" \"¬≤\" \"¬µ\" \"¬∑\" \"¬º\" \"¬Ω\" \"¬æ\" \"√ñ\" \"√ó\" \"√†\" \"√°\" \"√¢\" \"√£\" \"√§\" \"√®\" \"√©\" \"√≠\" \"√Ø\" \"√±\" \"√≥\" \"√¥\" \"√∂\" \"√∑\" \"√º\" \"≈ô\" \"≈°\" \"Œì\" \"Œî\" \"Œï\" \"Œò\" \"Œ±\" \"Œ≤\" \"Œµ\" \"Œº\" \"‡∏Å\" \"‡∏Ç\" \"‡∏É\" \"‡∏Ñ\" \"‡∏Ö\" \"‡∏Ü\" \"‡∏á\" \"‡∏à\" \"‡∏â\" \"‡∏ä\" \"‡∏ã\" \"‡∏å\" \"‡∏ç\" \"‡∏é\" \"‡∏è\" \"‡∏ê\" \"‡∏ë\" \"‡∏í\" \"‡∏ì\" \"‡∏î\" \"‡∏ï\" \"‡∏ñ\" \"‡∏ó\" \"‡∏ò\" \"‡∏ô\" \"‡∏ö\" \"‡∏õ\" \"‡∏ú\" \"‡∏ù\" \"‡∏û\" \"‡∏ü\" \"‡∏†\" \"‡∏°\" \"‡∏¢\" \"‡∏£\" \"‡∏§\" \"‡∏•\" \"‡∏ß\" \"‡∏®\" \"‡∏©\" \"‡∏™\" \"‡∏´\" \"‡∏¨\" \"‡∏≠\" \"‡∏Æ\" \"‡∏Ø\" \"‡∏∞\" \"‡∏±\" \"‡∏≤\" \"‡∏≥\" \"‡∏¥\" \"‡∏µ\" \"‡∏∂\" \"‡∏∑\" \"‡∏∏\" \"‡∏π\" \"‡∏∫\" \"‡∏ø\" \"‡πÄ\" \"‡πÅ\" \"‡πÇ\" \"‡πÉ\" \"‡πÑ\" \"‡πÖ\" \"‡πÜ\" \"‡πá\" \"‡πà\" \"‡πâ\" \"‡πä\" \"‡πã\" \"‡πå\" \"‡πç\" \"‡πê\" \"‡πë\" \"‡πí\" \"‡πì\" \"‡πî\" \"‡πï\" \"‡πñ\" \"‡πó\" \"‡πò\" \"‡πô\" \"‚Äã\" \"‚Äé\" \"‚Äè\" \"‚Äê\" \"‚Äì\" \"‚Äî\" \"‚Äò\" \"‚Äô\" \"‚Äú\" \"‚Äù\" \"‚Ä¢\" \"‚Ä¶\" \"‚Ä™\" \"‚Ä≤\" \"‚Ä≥\" \"‚Äº\" \"‚Å£\" \"‚ÇÇ\" \"‚ÑÉ\" \"‚Ñ¢\" \"‚Öì\" \"‚Üí\" \"‚â†\" \"‚â§\" \"‚â•\" \"‚ñ™\" \"‚ñ∂\" \"‚óè\" \"‚ô•\" \"‚ô¶\" \"‚úÖ\" \"‚úî\" \"‚ú®\" \"‚ù§\" \"„ÅÑ\" \"„ÅÜ\" \"„Åã\" \"„Åç\" \"„Åè\" \"„Åë\" \"„Åí\" \"„Åî\" \"„Åó\" \"„Åò\" \"„Åô\" \"„Åü\" \"„Å©\" \"„Å™\" \"„Å≤\" \"„Å∂\" \"„Åæ\" \"„Åø\" \"„ÇÅ\" \"„Çâ\" \"„Çä\" \"„Çã\" \"„Çå\" \"„Çè\" \"„Çì\" \"„Ç¢\" \"„Ç§\" \"„Ç¶\" \"„Ç™\" \"„Ç´\" \"„Ç≠\" \"„ÇÆ\" \"„ÇØ\" \"„Ç≥\" \"„Ç¥\" \"„Çµ\" \"„Ç∂\" \"„Ç∑\" \"„Ç∏\" \"„Çπ\" \"„Çø\" \"„ÉÅ\" \"„ÉÑ\" \"„ÉÜ\" \"„Éà\" \"„Éä\" \"„Éã\" \"„Éé\" \"„Éê\" \"„Éí\" \"„Éï\" \"„Éñ\" \"„Éô\" \"„Éõ\" \"„Éú\" \"„Éû\" \"„Éü\" \"„É†\" \"„É°\" \"„É¢\" \"„É£\" \"„É§\" \"„É©\" \"„É™\" \"„É´\" \"„É¨\" \"„É≠\" \"„ÉØ\" \"„É≥\" \"„Éº\" \"„Üç\" \"‰∏ú\" \"‰∏∞\" \"‰∫¨\" \"ÂÖÜ\" \"ÂÖ•\" \"Âåñ\" \"Âåó\" \"Âåª\" \"Âêà\" \"Â§è\" \"Â§ß\" \"Â≠¶\" \"ÂÆù\" \"Â∏É\" \"Âπ¥\" \"ÊâÄ\" \"ÊòÜ\" \"ÊõΩ\" \"Êú¨\" \"Êùâ\" \"Êù±\" \"Êüø\" \"Ê†π\" \"Ê∞¥\" \"Êµ¥\" \"ÁÅµ\" \"Áâ©\" \"ÁôΩ\" \"Á†î\" \"Á¢≥\" \"Á©∂\" \"Á∑è\" \"Á∫¢\" \"ËÇ§\" \"ËÇ™\" \"ËÉΩ\" \"ËÑÇ\" \"Ëä±\" \"ËäΩ\" \"Ëé≤\" \"Ëõã\" \"Ë±Ü\" \"Ë¥®\" \"ÈÉΩ\" \"ÈÖ¢\" \"Èáè\" \"Èí†\" \"Èõ™\" \"Èªí\" \"Èº†\" \"Ô¨Å\" \"Ô∏é\" \"Ô∏è\" \"Ôªø\" \"ÔΩû\" \"üåé\" \"üå∏\" \"üå∫\" \"üåø\" \"üçÄ\" \"üçÑ\" \"üéØ\" \"üèÜ\" \"üè∑\" \"üëâ\" \"üí•\" \"üìå\" \"üî•\" \"üî∫\" \"üòÜ\" \"üòä\" \"üò≠\" \"üôã\" \"üôè\" \"üõé\" \"ü§£\" \"ü•∞\" \"ü•µ\" "
     ]
    }
   ],
   "source": [
    "char = set()\n",
    "# for k, v in dataset.items():\n",
    "#     for text in v:\n",
    "#         char = char.union(set(\"\".join(text[\"Title\"])).union(set(\"\".join(text[\"Detail\"]))))\n",
    "for line in dataset:\n",
    "    char = char.union(set(line))\n",
    "char = sorted(char)\n",
    "\n",
    "print(\"All characters:\\n\")\n",
    "_ = [print(f'\"{c}\"', end=\" \") for c in char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c3d78d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mraw_text\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
     ]
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00af699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthcare statistics\n",
      "count    23716.000000\n",
      "mean       285.999030\n",
      "std        374.752765\n",
      "min          0.000000\n",
      "25%         13.000000\n",
      "50%         29.000000\n",
      "75%        473.000000\n",
      "max       9260.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Healthcare statistics\")\n",
    "\n",
    "text_len = pd.Series([len(t) for t in raw_text])\n",
    "print(text_len.describe())\n",
    "plt.hist(text_len, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39e15c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki TH statistics\n",
      "count    1.311984e+06\n",
      "mean     1.694320e+02\n",
      "std      2.297276e+02\n",
      "min      2.000000e+00\n",
      "25%      1.700000e+01\n",
      "50%      5.600000e+01\n",
      "75%      2.560000e+02\n",
      "max      9.797000e+03\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Wiki TH statistics\")\n",
    "\n",
    "text_len = pd.Series([len(t) for t in open(\"dataset/wiki_20210620_clean.txt\", \"r\").readlines()])\n",
    "print(text_len.describe())\n",
    "plt.hist(text_len, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42b2c4",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebda57d",
   "metadata": {},
   "source": [
    "## Train raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00783640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'airesearch/wangchanberta-base-att-spm-uncased'\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=416)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='cls')\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c172152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_objectives\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevaluator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceEvaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceEvaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'WarmupLinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'transformers.optimization.AdamW'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2e-05\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight_decay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevaluation_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_best_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcheckpoint_save_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcheckpoint_save_total_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Train the model with the given training objective\n",
       "Each training objective is sampled in turn for one batch.\n",
       "We sample only as many batches from each objective as there are in the smallest one\n",
       "to make sure of equal training with each dataset.\n",
       "\n",
       ":param train_objectives: Tuples of (DataLoader, LossFunction). Pass more than one for multi-task learning\n",
       ":param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n",
       ":param epochs: Number of epochs for training\n",
       ":param steps_per_epoch: Number of training steps per epoch. If set to None (default), one epoch is equal the DataLoader size from train_objectives.\n",
       ":param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
       ":param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n",
       ":param optimizer_class: Optimizer\n",
       ":param optimizer_params: Optimizer parameters\n",
       ":param weight_decay: Weight decay for model parameters\n",
       ":param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n",
       ":param output_path: Storage path for the model and evaluation files\n",
       ":param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n",
       ":param max_grad_norm: Used for gradient normalization.\n",
       ":param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n",
       ":param callback: Callback function that is invoked after each evaluation.\n",
       "        It must accept the following three parameters in this order:\n",
       "        `score`, `epoch`, `steps`\n",
       ":param show_progress_bar: If True, output a tqdm progress bar\n",
       ":param checkpoint_path: Folder to save checkpoints during training\n",
       ":param checkpoint_save_steps: Will save a checkpoint after so many steps\n",
       ":param checkpoint_save_total_limit: Total number of checkpoints to store\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec74ec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b5ea342f004ab2a5784baf7f533509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641e418027c34bdd8c969e1328d6af22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mMultipleNegativesRankingLoss(model)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m##  fit model ##\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/simcse-model-wangchanberta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights/wangchanberta-simcse-raw-bs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:689\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    686\u001b[0m data_iterator \u001b[38;5;241m=\u001b[39m data_iterators[train_idx]\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 689\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataloaders[train_idx])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:541\u001b[0m, in \u001b[0;36mSentenceTransformer.smart_batching_collate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    539\u001b[0m sentence_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_texts):\n\u001b[0;32m--> 541\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     batch_to_device(tokenized, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_device)\n\u001b[1;32m    543\u001b[0m     sentence_features\u001b[38;5;241m.\u001b[39mappend(tokenized)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:318\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:113\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n\u001b[1;32m    111\u001b[0m     to_tokenize \u001b[38;5;241m=\u001b[39m [[s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m to_tokenize]\n\u001b[0;32m--> 113\u001b[0m output\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_tokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongest_first\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2477\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2474\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2475\u001b[0m         )\n\u001b[1;32m   2476\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2498\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2499\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2515\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2516\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2668\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2659\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2660\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2661\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2666\u001b[0m )\n\u001b[0;32m-> 2668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:425\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    418\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    419\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    423\u001b[0m )\n\u001b[0;32m--> 425\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    437\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    439\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    449\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# notebook lib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = \"dataset\"\n",
    "NEWS_DIR = f\"{DATA_DIR}/healthcare-news\"\n",
    "DATASET = \"raw\"\n",
    "# DATASET = \"sentencepiece\"\n",
    "# DATASET = \"wordpiece\"\n",
    "\n",
    "\n",
    "def load_data(set_name):\n",
    "    if set_name.lower().strip() == \"wordpiece\":\n",
    "        json_paths = sorted(glob(f\"{NEWS_DIR}/wordpiece/*.json\"))\n",
    "        return {\n",
    "            os.path.basename(f_name).split(\"_\")[0]: [\n",
    "                json.loads(line.strip())\n",
    "                for line in open(f_name).readlines()\n",
    "            ]\n",
    "            for f_name\n",
    "            in tqdm(json_paths)\n",
    "        }\n",
    "    elif set_name.lower().strip() == \"sentencepiece\":\n",
    "        return [l.strip() for l in open(f\"{NEWS_DIR}/sentencepiece/sentencepiece.txt\")]\n",
    "    elif set_name.lower().strip() == \"raw\":\n",
    "        return [l.strip() for l in open(f\"{NEWS_DIR}/healthcare-raw.txt\")]\n",
    "\n",
    "dataset = load_data(DATASET)\n",
    "for batch_size in [16, 24]:\n",
    "    for learning_rate in [3e-5, 7e-5, 1e-4]:\n",
    "        ## load model ##\n",
    "        # LM models (mBERT, XLM-R, etc.)\n",
    "        model_name = 'airesearch/wangchanberta-base-att-spm-uncased'\n",
    "        word_embedding_model = models.Transformer(model_name, max_seq_length=416)\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='cls')\n",
    "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "        ## load dataset ##\n",
    "        train_sentences = open(\"dataset/healthcare-news/healthcare-raw.txt\").readlines()\n",
    "        # Convert train sentences to sentence pairs\n",
    "        train_data = [InputExample(texts=[s, s]) for s in train_sentences]\n",
    "        # DataLoader to batch your data\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Use the denoising auto-encoder loss\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "        ##  fit model ##\n",
    "        model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=1,\n",
    "            show_progress_bar=True,\n",
    "            optimizer_params={'lr': learning_rate},\n",
    "            output_path='output/simcse-model-wangchanberta',\n",
    "            save_best_model=True,\n",
    "            use_amp=True\n",
    "        )\n",
    "\n",
    "        model.save(f\"weights/wangchanberta-simcse-raw-bs{batch_size}-lr{learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f309c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
